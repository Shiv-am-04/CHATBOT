{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOhLFWYSaGsHYPZXgN9X04"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "corpus_movie_conv = r'/content/movie_conversations.txt'\n",
        "corpus_movie_lines = r'/content/movie_lines.txt'"
      ],
      "metadata": {
        "id": "xYq2MR9Q-Mcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-nlp"
      ],
      "metadata": {
        "id": "7JMyGh0sPxnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.layers import TokenAndPositionEmbedding"
      ],
      "metadata": {
        "id": "NN56z8RoP_US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "\n",
        "tensorflow.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9Js_603CQHZX",
        "outputId": "e235c738-856a-4faa-8fb1-af9ea3e186b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W78MlWkSWlJn",
        "outputId": "10507fe4-53ba-4efd-f784-51bc833f07d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(corpus_movie_conv, 'r') as f:\n",
        "    conversation = f.readlines()"
      ],
      "metadata": {
        "id": "zY9Vs9HB-Owg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(corpus_movie_lines, 'r',encoding='latin-1') as c:\n",
        "    dialogues = c.readlines()"
      ],
      "metadata": {
        "id": "ddupu-rJ_cL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation[0].split(' +++$+++ ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wA0Q42OAOGX",
        "outputId": "3352fdd7-d580-4d7b-e48a-cff0ac4a5c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['u0', 'u2', 'm0', \"['L194', 'L195', 'L196', 'L197']\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues[0].split(' +++$+++ ')[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YlKXCE_MAmLF",
        "outputId": "a536f239-2212-494a-cd74-2f7627df8d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'They do not!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines_dict = {}\n",
        "\n",
        "for lines in dialogues:\n",
        "  lines = lines.split(' +++$+++ ')\n",
        "  line_number = lines[0]\n",
        "  line = lines[-1]\n",
        "  lines_dict[line_number] = line"
      ],
      "metadata": {
        "id": "LpU1RfSQCCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_dict"
      ],
      "metadata": {
        "id": "LX3tjS78DbMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  without_punc = \"\"\n",
        "  for char in text:\n",
        "    if char not in punctuations:\n",
        "      without_punc = without_punc+char\n",
        "\n",
        "  return without_punc.lower()"
      ],
      "metadata": {
        "id": "VezCnddXDlk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punctuations('kdkwmoo@#$')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "A3Z7q4gPD4Rf",
        "outputId": "6a7d8661-38b5-44c7-f742-94b2db6408c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kdkwmoo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 30\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for conv in conversation:\n",
        "  conv = eval(conv.split(' +++$+++ ')[-1])\n",
        "\n",
        "  for i in range(len(conv)-1):\n",
        "    temp = []\n",
        "    line1 = remove_punctuations(lines_dict[conv[i]]).strip()\n",
        "    line2 = remove_punctuations(lines_dict[conv[i+1]]).strip()\n",
        "    temp.append(line1.split()[:maxlen])\n",
        "    temp.append(line2.split()[:maxlen])\n",
        "    pairs.append(temp)"
      ],
      "metadata": {
        "id": "0cbG5_KUD_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0]"
      ],
      "metadata": {
        "id": "qiwplWzmH0hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikhv1NeBKMAg",
        "outputId": "d53c2aa9-87b8-4c19-d630-085839901873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221616"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Word 2 Index mapping*"
      ],
      "metadata": {
        "id": "Ph2gTCiNTA-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "kdkcYR_ALwlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# help(Counter)"
      ],
      "metadata": {
        "id": "WeAYWkc0L_KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "\n",
        "for pair in pairs:\n",
        "  counter.update(pair[0])\n",
        "  counter.update(pair[1])"
      ],
      "metadata": {
        "id": "ffq6KqguMYjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in counter.keys() if counter[word]>5]\n",
        "\n",
        "# those words which occurs more than 5 times"
      ],
      "metadata": {
        "id": "8hfkASiOQYxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "lhGA2T8ZRJkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2indx = {}\n",
        "\n",
        "for i,word in enumerate(words):\n",
        "  word2indx[word] = i+1"
      ],
      "metadata": {
        "id": "udlvHv-ARNec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2indx['<UNKW>'] = len(word2indx) + 1\n",
        "word2indx['<start>'] = len(word2indx) + 1\n",
        "word2indx['<end>'] = len(word2indx) + 1\n",
        "word2indx['<pad>'] = 0"
      ],
      "metadata": {
        "id": "vokmYnJQRp5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*encoding and padding*"
      ],
      "metadata": {
        "id": "E_8wLEBZS8rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_questions(words,word2indx):\n",
        "  encoded = [word2indx.get(word,word2indx['<UNKW>']) for word in words] + [word2indx['<pad>']]*abs(maxlen-len(words))\n",
        "\n",
        "  return encoded"
      ],
      "metadata": {
        "id": "qZ1z_JWpSEGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_reply(words,word2indx):\n",
        "  encoded = [word2indx['<start>']] + [word2indx.get(word,word2indx['<UNKW>']) for word in words] + [word2indx['<end>']] + [word2indx['<pad>']]*abs(maxlen-len(words))\n",
        "\n",
        "  return encoded"
      ],
      "metadata": {
        "id": "pvpbfY03TuPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_reply(pairs[0][1],word2indx)"
      ],
      "metadata": {
        "id": "UgJHDt0bUOR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_pairs = []\n",
        "\n",
        "for pair in pairs:\n",
        "  Q = encode_questions(pair[0],word2indx)\n",
        "  R = encode_reply(pair[1],word2indx)\n",
        "  encoded_pairs.append([Q,R])"
      ],
      "metadata": {
        "id": "YGwRO7R2irSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "mmGd37dVUXsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "3-5yu38AYA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.pairs = encoded_pairs\n",
        "    self.dataset_size = len(pairs)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    question = torch.LongTensor(self.pairs[index][0])\n",
        "    reply = torch.LongTensor(self.pairs[index][1])\n",
        "\n",
        "    return question,reply\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.dataset_size"
      ],
      "metadata": {
        "id": "hUDB6DsNZKT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DataLoader***\n",
        "\n",
        "*DataLoader automatically splits the dataset into smaller batches of data, which can be processed in parallel*\n",
        "\n",
        "*DataLoader can load data in parallel using multiple worker threads (specified by the num_workers parameter). This speeds up the data loading process, especially when working with large datasets*"
      ],
      "metadata": {
        "id": "9m5HGNsEbuCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(Dataset(),batch_size=32,shuffle=True,num_workers=2,pin_memory=True)"
      ],
      "metadata": {
        "id": "iuHIkCP9aac2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Masking*"
      ],
      "metadata": {
        "id": "apG9WCbrcZf9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZQiQs00cbc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.triu(torch.ones(4,4)).transpose(0,1).unsqueeze(0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNKfy9JbpzZa",
        "outputId": "8955f4dd-369f-4d02-8235-bb6fafb9a2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masking(question,reply_input,reply_target):\n",
        "  # \"subsequent mask\" to ensure that when predicting a word in a sequence, the model only looks at the current and previous words, not future ones.\n",
        "  def subsequent_mask(size):\n",
        "    mask_matrix = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "\n",
        "    return mask_matrix.unsqueeze(0)\n",
        "\n",
        "    # this will create upper triangluar matrix\n",
        "    # [1,0,0,0]\n",
        "    # [1,1,0,0]\n",
        "    # [1,1,1,0]\n",
        "    # [1,1,1,1]\n",
        "\n",
        "  # masking position of words as 1 and rest as 0\n",
        "  question_mask = (question!=0)\n",
        "  # adding dimension: original --> (batch_size,num_words)    after --> (batch_size,1,1,num_words)\n",
        "  question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "  reply_input_mask = (reply_input!=0)\n",
        "  reply_input_mask = reply_input_mask.unsqueeze(1)\n",
        "  reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
        "  reply_input_mask = reply_input_mask.unsqueeze(1)\n",
        "\n",
        "  reply_target_mask = reply_target!=0\n",
        "\n",
        "  return question_mask,reply_input_mask,reply_target_mask"
      ],
      "metadata": {
        "id": "izl_PFO8oq9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Embeddings*"
      ],
      "metadata": {
        "id": "PAr63eEMrA3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import math"
      ],
      "metadata": {
        "id": "RjVah1VOrAmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
        "\n",
        "nn.Embedding(12, 4)(input_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ah3MOQsGks",
        "outputId": "d1ac7375-b036-4ce5-8608-b1f2b748a973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.3097,  1.0914,  0.7815,  0.9753],\n",
              "         [ 0.4986,  1.5327,  1.3669,  0.3893],\n",
              "         [ 0.3557,  1.0900, -0.2915,  0.3765],\n",
              "         [-2.4833, -1.4254,  0.2277,  1.2642]],\n",
              "\n",
              "        [[ 0.3557,  1.0900, -0.2915,  0.3765],\n",
              "         [ 0.2111,  0.1213, -0.8335,  1.9618],\n",
              "         [ 0.4986,  1.5327,  1.3669,  0.3893],\n",
              "         [-0.4673,  0.2228, -0.4245,  1.7784]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.randn(2, 4)\n",
        "\n",
        "nn.Linear(4,5)(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzIIBxM3tP-A",
        "outputId": "34c7cdde-2903-4d85-8342-d5c7cb92159e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2552,  0.7170, -1.2658, -1.4811, -1.0806],\n",
              "        [-0.4875, -0.3928, -0.4954,  0.4320,  0.6854]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# help(nn.Module)"
      ],
      "metadata": {
        "id": "xjfQ4s7LrGku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  '''\n",
        "  embeddings of words and adding positional encoding to them\n",
        "  '''\n",
        "  def __init__(self,vocab_size,dim_model,max_len=50):\n",
        "    super(Embeddings,self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.embeddings = nn.Embedding(vocab_size,dim_model)\n",
        "    self.pos_encoding = self.positional_encoding(max_len,dim_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def positional_encoding(self,max_len,dim_model):\n",
        "    pos_encoding = torch.zeros(max_len,dim_model)\n",
        "    for pos in range(max_len):\n",
        "      for i in range(0,dim_model,2):\n",
        "        pos_encoding[pos,i] = math.sin(pos/10000**((2*i)/dim_model))\n",
        "        pos_encoding[pos,i+1] = math.cos(pos/10000**((2*(i+1))/dim_model))\n",
        "\n",
        "    pos_encoding = pos_encoding.unsqueeze(0)            # adding dimension for batch size\n",
        "\n",
        "    return pos_encoding\n",
        "\n",
        "  def forward(self,encoded_word):\n",
        "    '''\n",
        "    this function add embeddings and positional encodings of words\n",
        "    '''\n",
        "    word_embeddings = self.embeddings(encoded_word)*math.sqrt(self.dim_model)\n",
        "    word_embeddings += self.pos_encoding[:,:word_embeddings.shape[1]]\n",
        "    word_embeddings = self.dropout(word_embeddings)\n",
        "\n",
        "    return word_embeddings"
      ],
      "metadata": {
        "id": "muMXXBYorOpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Multihead Attention*"
      ],
      "metadata": {
        "id": "FGBWg_IgqcZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dim_model,heads):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    assert dim_model%heads == 0\n",
        "    self.dim_head = dim_model//heads\n",
        "    self.heads = heads\n",
        "    self.dim_model = dim_model\n",
        "    self.query = nn.Linear(dim_model,dim_model)\n",
        "    self.key = nn.Linear(dim_model,dim_model)\n",
        "    self.value = nn.Linear(dim_model,dim_model)\n",
        "    self.concat = nn.Linear(dim_model,dim_model)               # this is to combine the attention scores of all the heads\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self,Q,K,V,mask):\n",
        "    '''\n",
        "    shape of query,key,value --> (batch_size,max_len,dim_model(=512))\n",
        "    mask shape --> (batch_size,1,1,num_words)\n",
        "    '''\n",
        "\n",
        "    Q = self.query(Q)\n",
        "    K = self.key(K)\n",
        "    V = self.value(V)\n",
        "\n",
        "    # (batch_size,max_len,512) --> (batch_size,max_len,head,each_head_dimension) --> (batch_size,heads,max_len,each_head_dimension)\n",
        "\n",
        "    Q = Q.view(Q.shape[0],-1,self.heads,self.dim_head).permute(0,2,1,3)\n",
        "    K = K.view(Q.shape[0],-1,self.heads,self.dim_head).permute(0,2,1,3)\n",
        "    V = V.view(Q.shape[0],-1,self.heads,self.dim_head).permute(0,2,1,3)\n",
        "\n",
        "\n",
        "    # calculating attention scores\n",
        "    # score = softmax((Q.KT)/sqrt(dim_head))\n",
        "    # weights = score.V\n",
        "\n",
        "    # (batch_size,heads,max_len,each_head_dimension) . (batch_size,heads,each_head_dimension,maxlen) = (batch_size,heads,max_len,max_len)\n",
        "    scores = torch.matmul(Q,K.permute(0,1,3,2))/math.sqrt(self.dim_head)\n",
        "    scores = scores.masked_fill(mask==0,-1e9)     #this replaces zero value with -1e9 so that softmax computes very less score for them as 0 are not useful for model.\n",
        "    scores = torch.softmax(scores,dim=-1)\n",
        "    scores = self.dropout(scores)\n",
        "\n",
        "    # (batch_size,heads,max_len,max_len) . (batch_size,heads,max_len,each_head_dimension) = (batch_size,heads,max_len,each_head_dimension)\n",
        "    weights = torch.matmul(scores,V)\n",
        "\n",
        "    # (batch_size,heads,max_len,each_head_dimension) --> (batch_size,max_len,heads,each_head_dimension) --> (batch_size,max_len,heads*each_head_dimension)\n",
        "    weights = weights.permute(0,2,1,3).contiguous().view(weights.shape[0],-1,self.heads*self.dim_head)\n",
        "\n",
        "    concated = self.concat(weights)\n",
        "\n",
        "    return concated"
      ],
      "metadata": {
        "id": "VCwjrESgqguL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Feed Forward Layer*"
      ],
      "metadata": {
        "id": "o-9zTgrhZSy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,dim_model,middle_dim=2048):\n",
        "    super(FeedForward,self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.layer1 = nn.Linear(dim_model,middle_dim)\n",
        "    self.layer2 = nn.Linear(middle_dim,dim_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self,input_):\n",
        "    x = torch.nn.functional.relu(self.layer1(input_))\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "2n9ksRM7CTA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4X_sAk-Talcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Encoder*"
      ],
      "metadata": {
        "id": "We-uEAoqbzuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,vocab_size,max_len,dim_model,heads):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.self_attention = MultiHeadAttention(self.dim_model,heads)\n",
        "    self.feed_forward = FeedForward(self.dim_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self,embeddings,mask):\n",
        "    self.layer_norm = nn.LayerNorm(self.dim_model)\n",
        "    encoded = self.self_attention(embeddings,embeddings,embeddings,mask)\n",
        "    encoded = self.layer_norm(encoded+embeddings)\n",
        "    feedforward = self.dropout(encoded)\n",
        "    feedforward = self.feed_forward(encoded)\n",
        "\n",
        "    final_encoded = self.layer_norm(encoded+feedforward)\n",
        "\n",
        "    return final_encoded"
      ],
      "metadata": {
        "id": "2TBGS-Ftb143"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Decoder*"
      ],
      "metadata": {
        "id": "KU6P48xziz-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,max_len,dim_model,heads):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.self_attention = MultiHeadAttention(self.dim_model,heads)\n",
        "    self.source_attention = MultiHeadAttention(self.dim_model,heads)\n",
        "    self.feed_forward = FeedForward(self.dim_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self,encoded,embeddings,source_mask,target_mask):\n",
        "    self.layer_norm = nn.LayerNorm(self.dim_model)\n",
        "    decoded_1 = self.self_attention(embeddings,embeddings,embeddings,target_mask)\n",
        "    decoded_1 = self.layer_norm(decoded_1+embeddings)\n",
        "\n",
        "    decoded = self.source_attention(decoded_1,encoded,encoded,source_mask)\n",
        "    decoded = self.layer_norm(decoded+decoded_1)\n",
        "\n",
        "    feedforward = self.dropout(decoded)\n",
        "    feedforward = self.feed_forward(decoded)\n",
        "\n",
        "    final_decoded = self.layer_norm(feedforward+decoded)\n",
        "\n",
        "    return final_decoded"
      ],
      "metadata": {
        "id": "TSJOzk_pi15x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Transformer*"
      ],
      "metadata": {
        "id": "qLFjT3ysK_5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,dim_model,max_len,heads,num_layers,word2index):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.vocab_size = len(word2index)\n",
        "    self.embeddings = Embeddings(self.vocab_size,self.dim_model)\n",
        "    self.encoder = nn.ModuleList([Encoder(self.vocab_size,max_len,self.dim_model,heads) for _ in range(num_layers)])\n",
        "    self.decoder = nn.ModuleList([Decoder(self.vocab_size,max_len,self.dim_model,heads) for _ in range(num_layers)])\n",
        "    self.logits = nn.Linear(self.dim_model,self.vocab_size)\n",
        "\n",
        "  def encode(self,question,question_mask):\n",
        "    question_embedding = self.embeddings(question)\n",
        "    for layer in self.encoder:\n",
        "      encode_embedding = layer(question_embedding,question_mask)\n",
        "\n",
        "    return encode_embedding\n",
        "\n",
        "  def decode(self,reply_target,encode_embedding,question_mask,reply_target_mask):\n",
        "    target_embedding = self.embeddings(reply_target)\n",
        "    for layer in self.decoder:\n",
        "      target_embedding = layer(encode_embedding,target_embedding,question_mask,reply_target_mask)\n",
        "\n",
        "    return target_embedding\n",
        "\n",
        "  def forward(self,question,question_mask,reply_target,reply_target_mask):\n",
        "    encoded = self.encode(question,question_mask)\n",
        "    decoded = self.decode(reply_target,encoded,question_mask,reply_target_mask)\n",
        "    logits = self.logits(decoded)\n",
        "    output = nn.functional.log_softmax(logits,dim=2)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "kiKUl7MKK4Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Adam Warmup*"
      ],
      "metadata": {
        "id": "x2y_l8XCt-aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWarmup:\n",
        "  def __init__(self,dim_model,warmup_steps,optimizer):\n",
        "    self.dim_model = dim_model\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.current_step = 0\n",
        "    self.learning_rate = 0\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "  def get_rate(self):\n",
        "    return (1/math.sqrt(self.dim_model))*min(1/math.sqrt(self.current_step),self.current_step*(1/math.sqrt(self.warmup_steps**3)))\n",
        "\n",
        "  def step(self):\n",
        "    self.current_step += 1\n",
        "    rate = self.get_rate()\n",
        "    for p in self.optimizer.param_groups:\n",
        "      p['lr'] = rate\n",
        "\n",
        "    self.learning_rate = rate\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "iSXl3PBEuBJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*KL Divergence Loss*"
      ],
      "metadata": {
        "id": "GzLg9HJ-wkhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KL_Loss(nn.Module):\n",
        "  def __init__(self,size_of_vectors,alpha):\n",
        "    super(KL_Loss,self).__init__()\n",
        "    self.loss = nn.KLDivLoss()\n",
        "    self.size_of_vectors = size_of_vectors\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def minimize_loss(self,predictions,target,target_mask):\n",
        "    '''\n",
        "    predictions --> (batch_size,max_words,vocab_size)\n",
        "    target --> (batch_size,max_words)\n",
        "    target_mask --> (batch_size,1,1,max_words)\n",
        "    '''\n",
        "    predictions = predictions.view(-1,predictions.shape[-1])       # (batch_size,max_words,vocab_size) --> (batch_size*max_words,vocab_size)\n",
        "    target = target.contiguous().view(-1)                         # (batch_size,max_words) --> (batch_size*max_words)\n",
        "    target_mask = target_mask.float()\n",
        "    target_mask = target_mask.view(-1)                            # (batch_size*max_words)\n",
        "    labels = predictions.data.clone()\n",
        "    labels = labels.fill_(self.alpha/self.size_of_vectors-1)\n",
        "    labels.scatter(1,target.data.unsqueeze(1),1-self.alpha)\n",
        "    # we have to change values against column i.e., 1st dimension that's why 1 and accessing the\n",
        "    # target index that's why we are using target but because scatter function takes second parameter as same size as labels and then at that\n",
        "    # particular index we replace value as 1 - alpha.\n",
        "\n",
        "    Loss = self.loss(predictions,labels)\n",
        "    Loss = (Loss*target_mask).sum()/target_mask.sum()\n",
        "\n",
        "    return Loss"
      ],
      "metadata": {
        "id": "oTyOnRj3wnxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Initializing transformer ,training and evaluating*"
      ],
      "metadata": {
        "id": "owLs9QRskSqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "iRNDy48rcIvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dimension = 512\n",
        "max_length = 50\n",
        "heads = 8\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_layers = 3\n",
        "\n",
        "transformer = Transformer(dim_model=model_dimension,max_len=max_length,heads=heads,num_layers=num_layers,word2index=word2indx)\n",
        "# transformer = transformer\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(dim_model=model_dimension,warmup_steps=4000,optimizer=adam_optimizer)\n",
        "kl_loss = KL_Loss(len(word2indx),0.2)"
      ],
      "metadata": {
        "id": "rnYZxYO_iPT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(data_loader,transformer,optimizer,loss,epochs):\n",
        "  transformer.train()\n",
        "  sum_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for i,(question,reply) in enumerate(data_loader):\n",
        "    samples = question.shape[0]\n",
        "\n",
        "    # reply input is without the <end> token\n",
        "    reply_input = reply[:,:-1]\n",
        "\n",
        "    # without <start> token\n",
        "    reply_target = reply[:,1:]\n",
        "\n",
        "    question = question\n",
        "    reply_input = reply_input\n",
        "    reply_target = reply_target\n",
        "\n",
        "    # initializing mask\n",
        "    question_mask,input_mask,target_mask = masking(question,reply_input,reply_target)\n",
        "\n",
        "    # forward propagation\n",
        "    output = transformer(question,question_mask,reply_input,input_mask)\n",
        "\n",
        "    # calculate loss\n",
        "    Loss = loss.minimize_loss(output,reply_target,target_mask)\n",
        "\n",
        "    # Back propagation\n",
        "    transformer_optimizer.optimizer.zero_grad()\n",
        "    Loss.backward()\n",
        "    transformer_optimizer.step()\n",
        "\n",
        "    sum_loss += Loss.item()*samples                     # .item() gives value rather than tensor\n",
        "    count += samples\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      # display(f\"Epoch [{epoch}][{i}/{len(data_loader)}]           Loss: {sum_loss/count:.3f}\")\n",
        "      tqdm.write(f\"Epoch [{epoch}][{i}/{len(data_loader)}] Loss: {sum_loss / count:.3f}\")"
      ],
      "metadata": {
        "id": "WxOMWMDVxADQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(transformer,question,question_mask,max_len,word2index):\n",
        "  transformer.eval()\n",
        "\n",
        "  # creating index to word mapping\n",
        "  indx2word = {}\n",
        "  for word,index in word2index.items():\n",
        "    indx2word[index] = word\n",
        "\n",
        "  start_token = word2index['<start>']\n",
        "  words = torch.LongTensor([[start_token]])\n",
        "  encoded = transformer.encode(question,question_mask)\n",
        "\n",
        "  for _ in range(max_len):\n",
        "    size = words.shape[1]\n",
        "    word_mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "    word_mask = word_mask.unsqueeze(0).unsqueeze(0)\n",
        "    decoded = transformer.decode(words,encoded,question_mask,word_mask)\n",
        "\n",
        "    prediction = transformer.logits(decoded[:,-1])\n",
        "    _,next_word = torch.max(prediction,dim=1)\n",
        "    next_word = next_word.item()\n",
        "\n",
        "    if next_word == word2index['<end>']:\n",
        "      break\n",
        "\n",
        "    words = torch.cat([words,torch.LongTensor([[next_word]])],dim=1)\n",
        "    # as more and more word are added words array increases -->(1,len(words))\n",
        "\n",
        "    # creating sentence\n",
        "    if words.dim() == 2:\n",
        "      words = words.squeeze(0)\n",
        "      words = words.tolist()\n",
        "\n",
        "    wordindex = [w for w in words if w not in {word2index['<start>'],word2index['<end>'],word2index['<pad>']}]\n",
        "\n",
        "    sentence = \" \".join([indx2word[wordindex[i]] for i in range(len(wordindex))])\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "eIuD9T0813va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train(data_loader,transformer,adam_optimizer,kl_loss,epoch)\n",
        "  state = {'epcoh':epoch,'transformer':transformer,'transformer_optimizer':transformer_optimizer}\n",
        "\n",
        "  torch.save(state,'checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "DVmESnhNlBKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0ae7ee-5c48-45f0-9ece-4f5672fe1788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0][0/6926] Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(1):\n",
        "  question = input('ask me anything! : ')\n",
        "  question = question.lower()\n",
        "  if question == 'quit' or question == 'Quit' or question == 'end':\n",
        "    break\n",
        "  encode_question = [word2indx.get(word,word2indx['<UNKW>']) for word in question.split()]\n",
        "  question = torch.LongTensor(encode_question).unsqueeze(0)\n",
        "  question_mask = (question!=0).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "  sentence = evaluate(transformer,question,question_mask,10,word2indx)\n",
        "  print(sentence)"
      ],
      "metadata": {
        "id": "AhWtZuiTgAhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}